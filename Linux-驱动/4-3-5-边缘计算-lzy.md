## 边缘计算

### 神经网络

#### 介绍

神经网络是一种人工智能的技术，可以用来识别图像、语音、文字等各种信息，也可以用来预测未来、生成创意、玩游戏等各种任务。但你知道神经网络是怎么工作的吗？它是不是真的像人的大脑一样，有无数的神经元相互连接，传递信号，进行思考呢？

其实，神经网络的原理并不复杂，它是一种数学模型，用来模拟人类神经系统的结构和功能。它由许多简单的单元组成，每个单元都可以接收一些输入，进行一些计算，然后输出一个结果。这些单元就像人类的神经元一样，可以相互连接，形成一个复杂的网络。通过这个网络，我们可以对输入的数据进行处理，得到我们想要的输出。

下面，我就来给你通俗地解释一下神经网络的结构和学习机制。

神经网络的基本结构如下图所示：

![img](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/0e2442a7d933c89543dffb342efa70fd800200d6.jpeg)

这个图中，每个圆圈代表一个单元，也叫做节点或神经元。每个节点都有一个或多个输入和一个输出。每条线代表一个连接，也叫做权重或突触。每个连接都有一个数值，表示这个连接的强度或重要性。每个节点还有一个偏置值，表示这个节点的自身特性或倾向。

我们可以看到，这个神经网络分为三层：输入层、隐藏层和输出层。输入层负责接收外界的数据，比如图片、声音、文字等。输出层负责输出我们想要的结果，比如分类、预测、生成等。隐藏层负责在输入层和输出层之间进行信息的转换和处理。隐藏层可以有多个，也可以没有。

我们可以用一个公式来表示每个节点的计算过程：

![image-20240725215214163](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240725215214163.png)

其中，

- output 是节点的输出值；
- f 是节点的激活函数，用来给节点引入非线性；
- wi 是第 i 条连接的权重值；
- inputi 是第 i 个输入值；
- bias 是节点的偏置值；
- ∑i=1n 表示对所有输入值进行加权求和。

激活函数是一种非线性函数，它可以让节点对不同范围和幅度的输入值有不同的响应。常见的激活函数有 Sigmoid 函数、tanh 函数、ReLU 函数等。下图展示了这些激活函数的图像：

我们可以看到，Sigmoid 函数和 tanh 函数都可以把任意范围的输入值映射到 (0, 1) 或 (-1, 1) 的范围内；ReLU 函数则可以把负数变为 0 ，正数保持不变。这些激活函数都可以让节点具有非线性特性，从而增强神经网络的表达能力。

#### 神经网络的学习机制

神经网络的学习机制是指如何调整网络中连接的权重值和节点的偏置值，使得网络能够对输入数据进行正确和有效地处理。通常，我们使用反向传播算法来实现神经网络的学习机制。

反向传播算法是一种基于梯度下降法的优化算法，它的基本思想是：

首先，给神经网络一个输入数据，让它进行前向传播，得到一个输出结果；

然后，将输出结果与期望的结果进行比较，计算出一个误差值；

接着，将误差值沿着网络反向传播，根据每个节点和连接对误差值的贡献程度，计算出每个节点和连接的梯度值；

最后，根据每个节点和连接的梯度值，对每个节点和连接的权重值和偏置值进行更新，使得误差值减小。

这个过程可以用下图来表示：

![img](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/64380cd7912397dda110c318a36bb1bad1a287b3.jpeg)

这个图中，我们假设神经网络只有一个隐藏层，输入层有两个节点，输出层有一个节点。我们给神经网络一个输入数据 [0.5, 0.8] ，期望的输出结果是 0.2 。我们可以看到，神经网络的实际输出结果是 0.7 ，与期望的结果相差 0.5  ，这就是误差值。我们用红色的箭头表示误差值沿着网络反向传播的过程。我们可以看到，每个节点和连接都有一个梯度值，表示它们对误差值的影响程度。我们用蓝色的箭头表示每个节点和连接的权重值和偏置值根据梯度值进行更新的过程。我们可以看到，每个节点和连接的权重值和偏置值都有一定程度的变化，使得误差值减小。

这个过程需要不断重复，直到误差值达到一个可接受的范围或者达到一个预设的次数。这样，神经网络就完成了学习过程，可以对新的输入数据进行处理了。

#### 常见的神经网络训练平台

**常见的神经网络训练工具包括TensorFlow、‌PyTorch、‌Keras和Caffe。‌**

- **TensorFlow**：‌由Google开发的开源深度学习框架，‌支持Python和C++编程语言。‌它提供了丰富的工具和库，‌使得用户可以轻松地构建和训练神经网络模型。‌TensorFlow支持各种类型的神经网络模型，‌包括卷积神经网络和循环神经网络等。‌
- **PyTorch**：‌由Facebook开发的轻量级深度学习框架，‌主要使用Python编程语言。‌PyTorch提供了动态计算图和张量操作等高级工具，‌使得模型的调试和修改更加方便。‌它也是一个受欢迎的选择，‌尤其是在学术和研究环境中。‌
- **Keras**：‌一个高级神经网络API，‌可以在TensorFlow、‌Theano和CNTK等后端引擎上运行。‌Keras的设计理念是简单易用，‌适合初学者快速上手。‌它支持Python和JavaScript等编程语言，‌并提供了一些实用的工具和库，‌如预训练模型和可视化工具等。‌
- **Caffe**：‌一个专注于卷积神经网络的深度学习框架，‌主要使用C++编程语言。‌Caffe具有高效的计算性能和易用的接口，‌虽然在计算机视觉领域得到了广泛应用，‌但其使用相对较窄。‌Caffe支持多种数据格式和硬件加速设备，‌提供了高度的可配置性和可扩展性。

#### **小结**

神经网络是一种人工智能的技术，它是由许多简单的单元组成的复杂网络，可以对各种数据进行处理。神经网络的结构由输入层、隐藏层和输出层组成，每个单元都有一个或多个输入、一个输出、一个激活函数、一个权重值和一个偏置值。神经网络的学习机制是通过反向传播算法来调整网络中单元和连接的权重值和偏置值，使得网络能够对输入数据进行正确和有效地处理。

### 边缘计算和云计算

在当今数字化时代，云计算和边缘计算作为两大重要的计算范式，正在推动着信息技术的快速发展。云计算和边缘计算代表了不同的数据处理和存储模型，各自在特定场景中发挥着重要作用。本文将深入探讨云计算与边缘计算之间的区别，着重介绍它们的定义、特点以及在各个领域的应用。

![img](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/v2-8a940dedfef852de4fd3a084ffc99dc5_1440w.jpg)

#### **云计算**

云计算是一种基于网络的计算模型，它通过将计算资源、存储服务、应用程序等提供给用户，使其能够通过互联网按需获得和使用这些资源。云计算的核心理念是将计算能力集中到大型数据中心，通过虚拟化技术实现资源的灵活分配和管理。

#### **云计算的特点**

- **虚拟化技术：** 云计算通过虚拟化技术实现硬件资源的抽象，使得用户无需关心底层硬件细节，能够更灵活地使用计算资源。
- **弹性和伸缩性：** 云计算允许用户根据需求快速扩展或缩减计算资源，实现资源的弹性使用，避免了资源浪费。
- **按需服务：** 用户可以按照需求购买和使用云计算提供的各种服务，无需预先投资大量资金建设自己的计算基础设施。

#### **边缘计算**

与云计算不同，边缘计算强调在数据产生源头或接近数据源的地方进行数据处理和存储。边缘计算旨在减少数据传输到中心数据中心的时间，降低延迟，提高响应速度，尤其适用于对延迟要求较高的场景。

#### **边缘特点**

- **低延迟：** 边缘计算通过在数据产生源头进行处理，减少了数据传输的距离，降低了数据传输的延迟，使得应用能够更快地响应用户请求。
- **数据处理在源头：** 边缘计算强调在数据产生的地方进行数据处理，避免了将大量数据传输到中心数据中心进行处理，减轻了网络负担。
- **适用于物联网：** 边缘计算特别适用于物联网场景，可以在设备端进行实时的数据分析和决策，提高了物联网系统的效率和可靠性。

#### **云计算与边缘计算：有什么区别？**

**数据处理位置的差异**

云计算强调将数据集中处理于中心数据中心，用户通过互联网访问和使用云提供的服务。相较之下，边缘计算将数据处理推向离数据源更近的边缘设备，如物联网设备、边缘服务器等，以实现更低延迟和更高效的数据处理。

- **云计算：** 用户上传照片至云存储，图像处理和分析发生在云端服务器。
- **边缘计算：** 智能摄像头在设备本地进行实时图像分析，而不是将全部数据传输到云端进行处理。

**延迟和响应时间**

云计算通常涉及将数据传输到远程数据中心进行处理，因此在数据传输和处理的过程中可能会出现较高的延迟。相反，边缘计算将数据处理推向更接近数据源的地方，使得在实时性要求较高的场景中能够更快速地做出响应。

- **云计算：** 云端语音识别系统可能有更长的响应时间，因为语音数据需要传输到云端进行处理。
- **边缘计算：** 边缘设备上的语音助手可以更迅速地响应用户指令，因为语音识别可以在本地进行。

**可用性和稳定性**

云计算通过大型数据中心提供服务，具有强大的计算和存储能力，但在某些情况下可能受到网络故障或数据中心故障的影响。边缘计算则通过分布在边缘设备上的计算资源提供服务，能够在某些情况下独立运行，提高了系统的可用性和稳定性。

- **云计算：** 在网络故障时，用户可能无法访问云端应用或服务。
- **边缘计算：** 智能传感器在断网时仍能够本地进行数据采集和处理，确保基础功能的可用性。

**应用场景的不同**

云计算更适用于需要大规模计算和存储的场景，例如大数据分析、人工智能训练等。而边缘计算更适用于对实时性和低延迟要求较高的场景，例如物联网、智能交通系统等。

- **云计算：** 通过云端服务器进行大规模的数据分析，例如社交媒体数据挖掘。
- **边缘计算：** 在智能城市中，交通信号灯通过边缘计算实时响应交通状况，优化信号灯控制。

**协同应用的优势**

云计算和边缘计算并非互斥，而是可以协同工作，充分发挥各自优势。通过将数据处理分布到边缘设备和云端数据中心之间，可以实现更灵活、高效的计算架构。以下是协同应用的一些优势：

- **低延迟和高带宽需求：** 边缘计算处理实时数据，而云计算处理更大规模的数据，二者相结合可以满足低延迟和高带宽需求。
- **资源优化：** 将部分计算任务移到边缘设备，可以减轻云端数据中心的负担，提高资源利用率。
- **数据安全性：** 对于一些敏感数据，可以在边缘设备上进行本地处理，减少数据传输，提高数据的安全性。

在智能工厂中，传感器和设备通过边缘计算实时收集和分析生产数据，以提高生产效率。同时，云计算可用于集中管理全局数据、进行长期分析和优化。

在医疗保健领域，可通过边缘计算实时监测患者生命体征，提供快速的紧急处理。云计算则可以用于存储和分析大规模的医疗数据，支持医学研究和精准医疗。

在智能交通系统中，边缘设备如交通摄像头和传感器可实时监测交通状况，快速做出响应。云计算则可以分析历史交通数据，优化交通流，提高城市交通效率。

AWS IoT Greengrass是亚马逊提供的边缘计算服务，允许设备在边缘运行本地计算，同时与云端服务进行协同工作。这使得设备可以在断网的情况下本地运行，并在恢复连接时与云端同步数据，提高了系统的稳定性。

Azure IoT Edge是微软提供的边缘计算服务，可以在边缘设备上运行容器化的应用程序。通过Azure IoT Edge，用户可以将部分工作负载移到边缘，减少云端的负载，同时实现低延迟的数据处理。

#### **总结**

云计算和边缘计算作为不同的计算范式，各自在特定场景中发挥着独特的作用。它们的区别体现在数据处理位置、延迟、可用性以及应用场景等方面。然而，随着数字化时代的发展，它们也逐渐形成了协同应用的趋势，充分发挥各自的优势，提供更灵活、高效的计算体系结构。

未来，随着智能化、自动化和边缘计算的边界拓展，云计算和边缘计算将进一步推动数字化转型。同时，安全性、跨边缘计算标准和环境可持续性等问题也需要在未来的发展中得到更好的解决。

### 神经网络处理器

#### NPU

NPU（Nerual Processing Unit）是一种专门用于加速神经网络计算的处理器。

在深度学习技术刚开始流行的时候，人们主要使用通用计算设备，如 CPU 和 GPU，来执行神经网络计算。但是，随着神经网络的复杂度和规模不断增加，传统的计算设备已经不能满足快速、高效地执行神经网络计算的需求。因此，研究人员开始探索如何设计一种专门用于加速神经网络计算的处理器，这就是 NPU 的由来。

早期的 NPU 主要是基于 FPGA（Field Programmable Gate Array可编程阵列逻辑）实现的。FPGA 是一种可编程逻辑器件，可以通过编程实现各种不同的电路功能。由于 FPGA 具有高度的灵活性和可编程性，可以用于实现各种不同类型的神经网络处理器。2010 年，斯坦福大学的研究人员提出了一种基于 FPGA 的神经网络加速器，可以实现高效的神经网络计算。此后，越来越多的研究人员开始尝试使用 FPGA 实现 NPU。

随着深度学习技术的不断发展和普及，NPU 的研究和开发也取得了越来越多的进展。2013年，Google 发布了一篇论文，介绍了一种名为“Tensor Processing Unit”（TPU）的定制芯片，专门用于加速深度学习模型的训练和推理。TPU 采用了特定的硬件架构和优化算法，可以实现高效、低功耗的神经网络计算。TPU 的成功引起了业界的广泛关注，也促进了 NPU 的发展。随后，其他公司也开始研发自己的 NPU。2015 年，华为发布了一款名为“Kirin 950”的处理器，集成了一种名为“Neural Processing Unit”（NPU）的模块，用于加速神经网络计算。2018年，华为推出了全新的“昇腾”（Ascend）系列处理器，其中包括专门用于加速深度学习计算的NPU 模块。同年，英伟达推出了名为“TensorCore”的加速器，用于加速深度学习模型的训练和推理。

随着 NPU 技术的不断发展和普及，越来越多的公司开始将 NPU 集成到自己的芯片中，以加速神经网络计算。例如，苹果公司在 2017 年发布的 A11 芯片中集成了神经网络处理器，用于支持人脸识别等功能。而瑞芯微为了满足人工智能的需要，瑞芯微的处理器也逐渐集成了NPU，而瑞芯微处理器内置的 NPU，就被称之为 RKNPU。

![image-20240725213338794](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240725213338794.png)

#### RKNPU 介绍

到目前为止，RKNPU 已经经过了几代的发展，趋近成熟。RK3399pro 和 RK1808 初次引入了 RKNPU，相比传统的 CPU 和 GPU 相比传统的 CPU 和 GPU，在深度学习运算能力上有比较大幅度的提升。接下来在 RV1109 和 RV1126 上使用了第二代 NPU，提升了 NPU 的利用率。第三代 NPU 应用在 RK3566 和 RK3568 上，搭载全新 NPU 自研架构，而 RK3588 搭载的为第四代 NPU,提高了带宽利用率，支持了多核扩展。RKNPU 具体发展过程如下图所示：

![image-20240725213510102](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240725213510102.png)

RKNPU1.0 和 RKNPU2.0 被划分为了 RKNPU，而 RKNPU3.0、RKNPU4.0 和 RKNPU5.0 被划分为了 RKNPU2,RKNPU 和 RKNPU2 所使用的 SDK 和工具套件不同，在后续的讲解中会再次进行说明。

**RK3568特性：**

（1）NPU 支持以下特性：

（2）使用 AHB 接口配置 NPU

（3）使用 AXI 接口从内存中获取数据

（4）支持 int8、int16、float16、Bfloat16 操作

（5）每个周期可进行 512 个 int8 MAC 操作

（6）每个周期可进行 128 个 int16 MAC 操作

（7）每个周期可进行 128 个 float16 MAC 操作

（8）每个周期可进行 128 个 bfloat16 MAC 操作

（9）每个核心有 256KB 的内部缓存

（10）支持转换的模型：TensorFlow、Caffe、Tflite、Pytorch、Onnx NN 等。

![image-20240725213625500](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240725213625500.png)

**AHB/AXI 接口**

AXI 主接口（AXI Master Interface）用于从连接到 SoC AXI 互联的内存中获取数据。AXI 是一种高性能、低延迟、可扩展的总线接口，常用于连接处理器和外设，并支持多个主设备和从设备。AXI 主接口通常用于从内存中获取数据，例如从 DRAM 或其他存储器中读取程序和数据，并将其传输到处理器或其他外设中进行处理和计算。

AHB 从接口（AHB Slave Interface）用于访问寄存器进行配置、调试和测试。AHB 是一种标准化的系统总线接口，通常用于连接处理器、内存和外设等硬件电路。AHB 从接口通常用于访问寄存器，例如控制和配置处理器、外设和其他硬件电路的参数和状态，以实现系统的配置、调试和测试。

在 SoC 系统中，AXI 主接口和 AHB 从接口通常被用于连接处理器、内存、外设和其他硬件电路，以实现数据传输、控制和配置等功能。AXI 接口通常用于高速数据传输和处理，而 AHB接口通常用于配置、调试和测试等低速控制操作。

**卷积神经网络加速单元（CNA）**

卷积神经网络加速单元（Convolutional Neural Network Accelerator，CNA）是 RKNPU 中重要的组成部分之一。包括卷积预处理控制器、NPU 内部缓存区、序列控制器、乘加运算单元和累加器，下面对各个部分进行介绍：

（1）卷积预处理控制器

卷积预处理控制器是 CNA 中用于预处理卷积计算的硬件单元，可以对输入的模型权重进行解压缩之后加载进 NPU 的内部缓冲区，并且可以判断零节点加速运算速度，最后将要推理的数据加载进 NPU 的内部缓冲区中。

（2）NPU 内部缓存区（Internal Buffer）

NPU 内部缓存区是 CNA 中用于存储中间计算结果的缓存区。它可以高效地存储和管理卷积神经网络中的各种数据，包括输入数据、卷积核、卷积结果等。NPU 内部缓存区采用了多级缓存和数据重用技术，可以高效地利用计算资源和存储资源，从而进一步提高计算速度和效率。

（3）序列控制器（Sequence Controller）

序列控制器是 CNA 中用于控制卷积计算序列的硬件单元。它可以根据卷积神经网络的结构和参数，自动地配置和控制 CNA 中各个硬件单元的工作模式和参数。序列控制器还可以实现卷积计算的并行化和流水化，从而提高计算速度和效率。

（4）乘加运算单元（Multiply-Accumulate Unit，MAC）

乘加运算单元是 CNA 中用于执行卷积计算的硬件单元。它可以对输入数据和卷积核进行乘法和累加运算，从而得到卷积计算结果。乘加运算单元采用了高度并行的设计，可以同时执行多个卷积计算操作，从而大大提高计算速度和效率。

（5）累加器（Accumulator）

累加器是 CNA 中用于累加卷积计算结果的硬件单元。它可以高效地累加卷积计算结果，从而得到最终的输出结果。累加器可以采用多种精度，可以适应不同的计算精度要求。

**数据处理单元（Data Processing Unit）**

数据处理单元（Data Processing Unit）主要处理单个数据的计算，例如 Leaky ReLU、ReLU、ReluX、Sigmoid、Tanh 等。它还提供了一些功能，例如 Softmax、转置、数据格式转换等。

数据处理单元是一种硬件电路，用于加速神经网络的计算过程。它通常被用于处理前向计算过程中的单个数据，例如卷积层和全连接层中的激活函数计算。不同的激活函数需要不同的计算操作，例如 ReLU 需要计算 max(0,x)，Sigmoid 需要计算 1/(1+exp(-x))，而数据处理单元可以通过硬件电路来实现这些计算操作，从而提高神经网络的计算性能和效率。

除了激活函数计算之外，数据处理单元还提供了一些其他的函数，例如 Softmax、转置、数据格式转换等。这些函数通常被用于神经网络模型的构建和优化过程中，例如将模型的输出转换为概率分布、重新排列张量的维度、将数据从一种格式转换为另一种格式等。

**平面处理单元（Planar Processing Unit）**

平面处理单元（Planar Processing Unit）主要提供对数据处理单元的输出数据进行平面操作的功能，例如平均池化、最大值池化、最小值池化等。

平面处理单元是一种硬件电路，用于加速神经网络的计算过程。它通常被用于对数据处理单元的输出数据进行平面操作，例如在卷积神经网络中，对卷积层的输出进行池化操作，以降低数据维度和减少计算量。平面处理单元可以通过硬件电路来实现这些操作，从而提高神经网络的计算性能和效率。

平面处理单元支持多种平面操作，例如平均池化、最大值池化、最小值池化等。这些操作可以通过不同的参数来控制池化窗口的大小和步幅等，从而适应不同的应用场景和要求。

#### NPU应用领域

RKNPU（Rockchip NPU）是一种专门用于深度学习应用的高性能处理器，并且在多个应用场景中都有广泛的应用。以下是一些常见的 RKNPU 应用场景：

![image-20240725213940106](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240725213940106.png)

### RKNPU 推理软件框架

#### 推理软件框架

![image-20240725215853957](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240725215853957.png)

**(1)RKNPU 硬件层**

关于 RKNPU 硬件层在上个章节已经进行了讲解，这里就不再进行过多的赘述。

**(2)RKNPU 驱动层**

RKNPU 的驱动层是连接上层应用和 RKNPU 硬件的桥梁。驱动层的主要作用是将应用程序需要推理的内容提交给 RKNPU 进行计算，从而加速神经网络的训练和推理过程。具体来说，

驱动层需要完成以下任务：

硬件初始化：驱动层需要初始化 RKNPU 硬件，包括设置寄存器、分配内存等操作，以确保 RKNPU 可以正常工作。

数据传输：驱动层需要将数据从主机内存传输到 RKNPU 内存中，以便进行计算。在计算完成后，驱动层还需要将计算结果传输回主机内存。

计算任务调度：驱动层需要根据应用程序的需求，管理和分配 RKNPU 的计算资源，以确保多个计算任务之间不会互相干扰。

**(3)RKNPU 应用层**

RKNPU 应用层由 AI 应用程序、RKNN API 以及运行时所需要的库所组成。

开发者通过调用瑞芯微提供好的 API 接口进行 AI 应用的开发，瑞芯微分别提供了 C 语言和Python 语言这两种 API 帮助开发者进行嵌入式设备部署，Python 语言提供的接口较为简单，旨在帮助用户进行前期的模型检测、测试以及应用调试，而要想得到更好的效果从而真正应用到实际项目中就要使用 C API 接口了。

而无论是由 C API 接口还是 Python 的 API 接口编写的应用程序，要想实际运行都需要相应的动态库，动态库包含这些 API 的具体实现，这些动态库由瑞芯微所提供。而我们只需要根据瑞芯微所提供的 API 编写对应的应用程序即可。

#### RKNN 模型

RKNN（Rockchip Neural Network）是瑞芯微公司开发的一种神经网络模型格式，它可以将常见的深度学习模型转换为适用于瑞芯微的 AI 加速器 RKNPU 的模型格式。RKNN 模型的优点是在保证精度的同时，可以实现高效的推理和低功耗的计算。下面是 RKNN 模型的详细介绍：

RKNN 模型的文件格式：RKNN 模型使用自定义的文件格式来表示神经网络模型。它将神经网络模型划分为两个部分：静态部分和动态部分。静态部分包括模型的网络结构和权重，而动态部分包括输入输出的形状和类型等信息。使用这种格式可以减少模型的存储空间和加载时间。

RKNN 模型的转换工具：为了将其他常见的深度学习模型转换为 RKNN 模型，瑞芯微提供了一个转换工具 RKNN-Toolkit2。该工具支持将 TensorFlow、Caffe、MXNet 等框架训练出来的模型转换为 RKNN 模型，并且支持对模型进行量化、融合等优化操作，以提高运行效率。

RKNN 模型的部署和推理：RKNN 模型可以通过 RKNPU 硬件进行高效的推理。在部署 RKNN模型时，开发者需要使用 RKNPU 提供的 SDK，并调用相应的 API 接口来加载和运行 RKNN 模型。由于 RKNPU 硬件的优化，RKNN 模型可以实现高效的推理和低功耗的计算。

![image-20240725220022396](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240725220022396.png)

#### RKNN Toolkit2

RKNN-Toolkit2 资料包中有四个目录，分别为 docs、examples、packages 和 rknn_toolkit_lite2，

docs 目录包含了 RKNN-Toolkit2 的使用文档，包括更新记录、两个 Python 环境所需的依赖项、RKNN 算子支持列表、快速入门指南以及使用指导手册。

examples 目录包含了 RKNN-Toolkit2 的一些示例代码，用于演示 RKNN-Toolkit2 的一些功能。通过运行该示例代码，可以帮助用户更好的理解和掌握 RKNN-Toolkit2 的具体功能和使用方法。

packages 目录提供了 Python3.6 和 Python3.8 两个版本的安装包，分别对应 X86 平台的ubuntu18 和 ubuntu20 两个发行版 Linux 操作系统。

而 rknn_toolkit_lite2 属于 RKNN-Toolkit2 的一个阉割版，会在下一小节进行讲解和对比。

下面对 RKNN-Toolkit2 进行简单的介绍：RKNN-Toolkit2 是瑞芯微为用户提供的、在 PC端 ubuntu 平台进行模型转换、推理和性能评估的开发套件，用户通过该工具提供的 Python 接口可以便捷地完成以下功能：

>  模型转换: 支持 Caffe、TensorFlow、TensorFlow Lite、ONNX、DarkNet、PyTorch 等模型转为 RKNN 模型，并支持 RKNN 模型导入导出，RKNN 模型能够在 Rockchip NPU 平台上加载使用
>
>  量化功能: 支持将浮点模型量化为定点模型，目前支持的量化方法为非对称量化 ( asymmetric_quantized-8 及 asymmetric_quantized-16 )，并支持混合量化功能 。 asymmetric_quantized-16 目前版本暂不支持
>
>  模型推理: 能够在 PC 上模拟 NPU 运行 RKNN 模型并获取推理结果或将 RKNN 模型分发到 Rockchip NPU 设备上进行推理并获取推理结果
>
>  性能评估和内存评估: 将 RKNN 模型分发到 Rockchip NPU 设备上运行，以评估模型在实际设备上运行时的性能和内存使用信息
>
>  量化精度分析: 该功能将给出模型量化前后每一层推理结果与浮点模型推理结果的余弦距离，以便于分析量化误差是如何出现的，为提高量化模型的精度提供思路
>
>  模型加密功能：使用指定的加密等级将 RKNN 模型整体加密。因为 RKNN 模型的加密是在 NPU 驱动中完成的，使用加密模型时，与普通 RKNN 模型一样加载即可，NPU 驱会自动对其进行解密。

#### RKNN Toolkit lite2

RKNN Toolkit lite2 相 较 于 RKNN-Toolkit2 多 了 一 个 lite 后 缀 ， 可 以 简 单 的 理 解 为RKNN-Toolkit2 的阉割版，只保留了推理的功能,可以帮助用户在开发板端进行模型的初步部署和测试。

RKNN-Toolkit lite2 资料包中有三个目录，分别为 docs、examples 和 packages。docs 目录包含了 RKNN-Toolkit lite2 的使用文档。

examples 目录包含了 RKNN-Toolkit lite2 的示例代码，由于 RKNN-Toolkit2 lite2 的使用较为简单所以只提供了一个示例。

packages 目录提供了 Python3.7 和 Python3.9 两个版本的安装包，需要注意的是这里的安装包架构为 aarch64，即需要安装在 RK3568 或者 RK3588 的 ubuntu 等发行版 Linux 操作系统上。

最后对 RKNN-Toolkit2 和 RKNN-Toolkit lite2 进行对比，对比如下：

（1）提供的语言接口都是 Python 语言。

（2）RKNN-Toolkit2 提供的安装包架构为 X86_64，运行在日常使用的 PC 电脑上，一般情况下安装在虚拟机 ubuntu 上，而 RKNN-Toolkit lite2 提供的安装包架构为 aarch64，运行在RK3568 或者 RK3588 开发板上。

（3）RKNN-Toolkit2 可以进行模型转换、推理和性能评估，而 RKNN-Toolkit lite2 只保留了推理功能，用于模型的初步部署和调试。

![image-20240725220419297](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240725220419297.png)

#### RKNPU2 SDK

RKNPU2 SDK 资料包中有 3 个目录，分别为 docs、examples 和 runtime。

docs 目录包含了 RKNPU2 SDK 的使用文档，包括 RKNN 算子支持列表、快速入门指南以及使用指导手册。

examples 目录包含了 RKNPU2 SDK 的一些示例代码和第三方库。通过这些示例代码，可以帮助我们更快速的掌握 RKNPU2 所提供的一些 C API 接口和使用流程。

runtime目录用于存放API运行时依赖的动态库和头文件，并且提供了一个名为rknn_server的可执行程序，在后面 RKNN-Toolkit2 连板推理时会用到。

RKNPU2 SDK 和 RKNN-Toolkit lite2 要实现的功能相同，都是在开发板部署 RKNN 模型，然后进行推理，只是相较于 RKNN-Toolkit lite2 提供了更多的接口，调用起来较为复杂，但可以获得更好的运行效果。

![image-20240725220454214](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240725220454214.png)

#### 学习步骤

无论使用 RKNPU2 SDK 提供的 C API 接口进行 RKNN 模型部署，还是使用 RKNN-Toolkit lite2提供的 Python API 接口进行 RKNN 模型部署，在此之前，都需要先使用 RKNN Toolkit2 将网络模型转换为 RKNN 模型，或者对模型进行精度分析等操作，所以在 RKNPU2 开发阶段，首先要学习 RKNN-Toolkit 2 的使用。

RKNN-Toolkit 2 的学习分为以下几个步骤，搭建 RKNN-Toolkit 2 开发环境、讲解 RKNN-Toolkit2 提供的 API 接口并测试常用功能、对 RKNN 模型构建、精度分析、性能评估、内存评估以及

RKNN 模型加载等重要接口进行单独讲解。

在 RKNPU 开发过程中，RKNN-Toolkit 2 就是用来构建并且评测 RKNN 模型的，得到符合我们大致预期的 RKNN 模型之后就可以进行项目的部署了，模型部署分为 RKNN-Toolkit lite2 部署和 RKNPU2 SDK 部署，由于 RKNN-Toolkit lite2 提供的接口与 RKNN-Toolkit 2 提供接口相似，且数量较少，主要用于初期调试，所以我们首先会讲解 RKNN-Toolkit lite2 部署，包括 RKNN-Toolkitlite2API 接口讲解、RKNN-Toolkit lite2 使用环境搭建、RKNN-Toolkit lite2 例程测试。

RKNN-Toolkit lite2 讲解完成之后就到了最难的 RKNPU2 SDK 提供的 C API 接口讲解了，C API根据处理数据时的方法可以分为 0 拷贝 API 和通用 API，首先会根据例程讲解两种 API 的使用框架、随后对瑞芯微提供的 C API 例程进行介绍，凭借这些例程可以帮助我们后期进行快速开发，在例程的讲解中会对出现的各个 C API 进行详细的讲解。

![image-20240725220613868](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240725220613868.png)

### RKNN Toolkit2 环境搭建

#### 安装conda

Conda 是一个开源的软件包管理系统和环境管理系统，它可以用于安装、管理和升级软件包和依赖项，我们这里使用 conda 的目的只是构建一个虚拟环境，所以选择轻量话的miniconda。miniconda 的官方链接如下所示：

https://docs.conda.io/en/latest/miniconda.html

进入 miniconda 的网址后如下所示：

可以看到下方有各个系统的安装包，我们选择 Miniconda3 Linux 64-bit 和 Miniconda3 Linux-aarch64 64-bit 两个版本的安装包进行下载.

本章节要用到的是 Miniconda3-latest-Linux-x86_64.sh 安装包，而 Miniconda3-latest-Linux-a

arch64.sh 安装包会用在之后 RKNN Toolkit lite2 环境搭建中。

首先将 Miniconda3-latest-Linux-x86_64.sh 安装包拷贝到虚拟机 ubuntu 上。

切换到普通用户

随后使用“./Miniconda3-latest-Linux-x86_64.sh”命令进行安装，根据提示，输入回车和“yes”，等待安装完成。

安装完成之后会自动设置环境变量，打开新的终端，发现用户名前出现（base），就代表安装成功了，如下图所示：

![image-20240726124631742](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240726124631742.png)

然后在用户目录下新建.condarc 文件，并输入以下内容：

```
channels: 
  - defaults
show_channel_urls: true
channel_alias: https://mirrors.tuna.tsinghua.edu.cn/anaconda
default_channels: 
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
custom_channels: 
  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
```

经过上面的步骤之后就会将 conda 镜像地址更改为清华源，可以极大的提高安装软件的速度。

#### 创建 RKNN 虚拟环境

为了避免环境后续学习中环境的冲突问题，这里使用 conda 创建 RKNN 虚拟环境，使用命令如下所示：

> conda create -n rknn python=3.8

命令执行之后，首先会要求安装一些列软件包，输入 y 确认即可.

然后使用“ conda activate rknn”激活相应得 rknn 环境，如下图所示：

![image-20240726124825321](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240726124825321.png)

可以看到，标识符由“base”变成了“rknn”,

#### 安装依赖软件和工具包

然后将 rknn_toolkit2-1.4.0_22dcfef4-cp38-cp38-linux_x86_64.whl 和 requirements_cp38-1.4.0.txt 文件拷贝到虚拟机 ubuntu 上。

首先使用以下命令安装 numpy ，安装过程如下图所示：

> pip3 install numpy==1.16.6 -i https://pypi.tuna.tsinghua.edu.cn/simple
>
> pip3 install numpy==1.16.6 -i https://mirrors.aliyun.com/pypi/simple

![image-20240726124951924](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240726124951924.png)

然后安装瑞芯微提供的 requirements_cp38-1.4.0.txt 文件所记录的依赖包，安装过程如下图所示：

>  pip3 install -r requirements_cp38-1.4.0.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
>
>  pip3 install -r requirements_cp38-1.4.0.txt -i https://mirrors.aliyun.com/pypi/simple
>
>  pip3 install -r requirements_cp38-1.6.0.txt -i https://mirrors.aliyun.com/pypi/simple

![image-20240726124959140](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240726124959140.png)

最后安装瑞芯微提供的 rknn_toolkit2-1.6.0 版本的软件包，安装完成如下图所示：

> pip3 install rknn_toolkit2-1.6.0+81f21f4d-cp38-cp38-linux_x86_64.whl -i https://mirrors.aliyun.com/pypi/simple

![image-20240726125025938](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240726125025938.png)

至此，RKNN 的虚拟环境就创建完成了。

#### 安装 pycharm

PyCharm 是 Python 最著名的集成开发环境 IDE 之一，由大名鼎鼎的 JetBrains 公司开发，如果你用过该公司其它产品，像 Intellij IDEA 或者 WebStorm，你将对 PyCharm 驾轻就熟，该公司旗下产品在功能布局及设置等方面都保持了很好的一致性。

PyCharm 的官网链接为“https://www.jetbrains.com/pycharm/".

使用以下命令对压缩包进行解压

> tar -vxf pycharm-community-2023.1.tar.gz

然后使用以下命令进入安装包的 bin 目录下,使用“./pycharm.sh”命令运行 Pycharm。

**创建快捷方式**

![image-20240726125308129](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240726125308129.png)

![image-20240726125316340](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240726125316340.png)

**配置中文**

![image-20240726125350406](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240726125350406.png)

**配置 RKNN 解释器**

点击“新建项目”，进入项目配置界面

![image-20240726125505024](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240726125505024.png)

这里选择解释器为“先前配置的解释器”，如下图所示：

![image-20240726125528476](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240726125528476.png)

然后点击添加本地解释器按钮，然后选择系统解释器，并点击路径选择按钮，

![image-20240726125558154](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240726125558154.png)

找到“/home/topeet/miniconda3/envs/rknn/bin/python3”并确定

![image-20240726125624443](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240726125624443.png)

创建完成之后，打开 Pycharm 内置终端，可以看到默认已经帮我们激活了 RKNN 虚拟环境，如下图所示

![image-20240726125644657](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240726125644657.png)

#### 可能遇到的问题

1. setuptools版本不对

   ![image-20240729103238505](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240729103238505.png)

   ```shell
   1. 卸载原来的setuptools
   pip uninstall setuptools
   2. 安装setuptools
   pip install setuptools==49.6.0 -i https://mirrors.aliyun.com/pypi/simple
   ```

   ![image-20240729104139749](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240729104139749.png)

   ```
   修改文件
   miniconda3/envs/rknn/lib/python3.8/site-packages/distutils-precedence.pth
   在import os;后面加一个换行
   ```

   ![image-20240729104448485](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240729104448485.png)

2. glibc版本不对

   ![image-20240729104703003](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240729104703003.png)
   
   1. 使用root身份, 解压glibc2.29的解压包
   
   2. 进入glibc安装包目录
   
   3. 创建一个build目录
   
   4. 进入build目录
   
   5. 配置
   
      ```
      ../configure --prefix=/usr/local --disable-sanity-checks
      ```
   
   7. 编译make
   
   8. 安装 make install
   
   9. 安装完成以后进入以下目录,ll查看动态库文件
   
      ```shell
      cd /lib/x86_64-linux-gnu
      ```
   
   10. 复安装的glibc2.29
   
       ```
       cp /usr/local/lib/libm-2.29.so /lib/x86_64-linux-gnu/
       ```
   
   11. 连接
   
       ```
       ln -sf libm-2.29.so libm.so.6
       ```
   
   ### 连板推理
   
   1. 复制需要文件
   
      ![image-20240729120603105](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240729120603105.png)
   
   2. ubuntu上安装adb
   
      ```
      sudo apt-get install adb
      ```
   
   3. 复制连板推理的服务端到开发板上
   
      ![image-20240729121326961](https://woniumd.oss-cn-hangzhou.aliyuncs.com/aiot/luozhaoyong/image-20240729121326961.png)
   
   4. 复制动态库文件到开发板上
   
      Linux/librknn_api/${BOARD_ARCH}/librknnrt.so到/usr/lib目录
   
   5. 运行
   
      ```
      ./start_rknn.sh
      ```
   
      
   
   

